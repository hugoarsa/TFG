{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing\n",
    "\n",
    "Now that all the metadata has been studied some testing with models from diferent sources and original code will be undergone in order to define some models to send to the boada environment.\n",
    "\n",
    "We will start slow with simple pre-trained models extracted directly from the pytorch environment and build from those up with the modifications that we want to study in the experimentation. Mainly the models tested will be ResNet (in some of it's variants) and some DensNet variants at first with the option of further models if everything goes to plan.\n",
    "\n",
    "Another objective would be to try to test the difference between transfer learning and full training from scratch, considering some other more sophisticated learning methods like one-shot if there is time.\n",
    "\n",
    "Finally some degree of localization will be accquired with the advancements of the paper from Selvaraju et al. of Grad-CAM.  Code will be recicled with the original paper code with the necessary modifications in order to apply it to the selected models of the project.\n",
    "\n",
    "\n",
    "## Preparing the split between test, train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def undersample_negatives(train_data, ratio=0.3):\n",
    "    \n",
    "    negative = train_data[train_data['No Finding'] == 1]\n",
    "    positive = train_data[train_data['No Finding'] == 0]\n",
    "\n",
    "    print(f'negative {len(negative)} positive {len(positive)}')\n",
    "    \n",
    "    negative = resample(negative, \n",
    "                        replace=False,\n",
    "                        n_samples=int(len(positive)*(ratio / (1- ratio))), # reduce as stated in ratio\n",
    "                        random_state=42)\n",
    "    \n",
    "    print(f'negative {len(negative)} positive {len(positive)}')\n",
    "    \n",
    "    return pd.concat([positive, negative])\n",
    "\n",
    "ratio = 0.3\n",
    "\n",
    "# Get the labels and read the original metadata\n",
    "labels = ['No Finding',\n",
    "          'Atelectasis',\n",
    "          'Cardiomegaly',\n",
    "          'Consolidation',\n",
    "          'Edema',\n",
    "          'Effusion',\n",
    "          'Emphysema',\n",
    "          'Fibrosis',\n",
    "          'Hernia',\n",
    "          'Infiltration',\n",
    "          'Mass',\n",
    "          'Nodule',\n",
    "          'Pleural_Thickening',\n",
    "          'Pneumonia',\n",
    "          'Pneumothorax']\n",
    "\n",
    "metadata = pd.read_csv('./labels/Data_Entry_2017_v2020.csv', delimiter=',')\n",
    "\n",
    "# Encode the labels with multi-label friendly encoding\n",
    "for label in labels:\n",
    "    metadata[label] = metadata['Finding Labels'].apply(lambda x: 1 if label in x else 0)\n",
    "\n",
    "metadata = metadata.drop(columns=['Finding Labels', 'Follow-up #','Patient Age', 'Patient Gender', 'View Position', 'OriginalImage[Width','Height]', 'OriginalImagePixelSpacing[x', 'y]'])\n",
    "\n",
    "# Get the test train and val splits according to the patient ID so no patients end up split between groups\n",
    "gss_test = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)\n",
    "train_val_idx, test_idx = next(gss_test.split(metadata, groups=metadata['Patient ID']))\n",
    "\n",
    "train_val_metadata = metadata.iloc[train_val_idx]\n",
    "test_metadata = metadata.iloc[test_idx]\n",
    "\n",
    "gss_train_val = GroupShuffleSplit(test_size=0.125, n_splits=1, random_state=42)\n",
    "train_idx, val_idx = next(gss_train_val.split(train_val_metadata, groups=train_val_metadata['Patient ID']))\n",
    "\n",
    "train_metadata = train_val_metadata.iloc[train_idx]\n",
    "val_metadata = train_val_metadata.iloc[val_idx]\n",
    "\n",
    "\n",
    "# Drop the column of patient ID\n",
    "train_metadata = train_metadata.drop(columns=['Patient ID'])\n",
    "val_metadata = val_metadata.drop(columns=['Patient ID'])\n",
    "test_metadata = test_metadata.drop(columns=['Patient ID'])\n",
    "\n",
    "\n",
    "# Undersample \"No Findings\"\n",
    "train_metadata = undersample_negatives(train_metadata,ratio)\n",
    "val_metadata = undersample_negatives(val_metadata,ratio)\n",
    "test_metadata = undersample_negatives(test_metadata,ratio)\n",
    "\n",
    "\n",
    "#Write all the new metadata as csv to load easier\n",
    "train_metadata.to_csv('./labels/train_metadata.csv', index=False)\n",
    "val_metadata.to_csv('./labels/val_metadata.csv', index=False)\n",
    "test_metadata.to_csv('./labels/test_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'in terms of all train: {len(train_metadata)}, val: {len(val_metadata)}, test: {len(test_metadata)} and total: {len(train_metadata) + len(test_metadata) + len(val_metadata) }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset definition and load (dataset.py)\n",
    "\n",
    "We define the dataset and load from the data entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "class ChestXRay(Dataset):\n",
    "    def __init__(self, df_dir, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df_dir: Path to the csv file with image names and labels.\n",
    "            image_dir: Directory with all the images with the labels.\n",
    "            transform: Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.classes = ['No Finding',\n",
    "                        'Atelectasis',\n",
    "                        'Cardiomegaly',\n",
    "                        'Consolidation',\n",
    "                        'Edema',\n",
    "                        'Effusion',\n",
    "                        'Emphysema',\n",
    "                        'Fibrosis',\n",
    "                        'Hernia',\n",
    "                        'Infiltration',\n",
    "                        'Mass',\n",
    "                        'Nodule',\n",
    "                        'Pleural_Thickening',\n",
    "                        'Pneumonia',\n",
    "                        'Pneumothorax']\n",
    "\n",
    "        \n",
    "        self.data_frame = pd.read_csv(df_dir)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.size = len(self.data_frame)\n",
    "        self.labels = np.array(self.data_frame.iloc[:, 1:])\n",
    "        self.images = self.data_frame.iloc[:,0]\n",
    "        self.class_count = self.labels.sum(0)\n",
    "        self.total_labels = self.class_count.sum()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image path\n",
    "        img_name = os.path.join(self.image_dir, self.images.iloc[idx])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        \n",
    "        # Get labels\n",
    "        labels = np.array(self.labels[idx])\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        image = np.array(image)\n",
    "        \n",
    "        return {'image': image, 'labels': labels}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various functions (utils.py)\n",
    "\n",
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                 [0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "def make_data_loaders(train_csv,val_csv,image_dir,batch_size,image_size):\n",
    "    \n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    ])\n",
    "\n",
    "    train_dataset = ChestXRay(df_dir=train_csv, image_dir=image_dir, transform=train_transforms)\n",
    "    val_dataset = ChestXRay(df_dir=val_csv, image_dir=image_dir, transform=val_transforms)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return {'train': train_loader, 'val': val_loader}, {'train':len(train_dataset),'val':len(val_dataset)}, train_dataset.class_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load and Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = './labels/train_metadata.csv'\n",
    "val_csv = './labels/val_metadata.csv'\n",
    "image_dir = './resized_images'\n",
    "\n",
    "dataloaders, dataset_sizes, class_counts = make_data_loaders(train_csv,val_csv,image_dir,128,256)\n",
    "\n",
    "print(class_counts)\n",
    "print(class_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def LoadModel(checkpoint_file, model, optimizer, epoch_inti, num_GPU):\n",
    "    '''\n",
    "    The loads the model, optimizer, current epoch, and current validation AUC from the checkpoint location provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    checkpoint_file: (str) the location of the model in s/m\n",
    "    model: PyTorch model\n",
    "    optimizer: PyTorch optimizer\n",
    "    epoch_inti: current epoch\n",
    "    num_GPU : (int) number of GPUs that are being used\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Returns the model, optimizer, epoch_inti, best_auc_ave from the saved location.\n",
    "    '''\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch_inti = checkpoint['epoch']\n",
    "    if num_GPU > 1:\n",
    "        model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model, optimizer, epoch_inti\n",
    "\n",
    "\n",
    "def SaveModel(model, optimizer, epoch, file_name, num_GPU):\n",
    "    \"\"\"\n",
    "    Save the model parameters, optimizer, best_AUC\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch : (int) current epoch\n",
    "    model : Pytorch model to save\n",
    "    optimizer : Pytorch optimzer to save\n",
    "    file_name : (str) location where the model needed to be saved\n",
    "    num_GPU : (int) number of GPUs that are being used\n",
    "\n",
    "    \"\"\"\n",
    "    if num_GPU > 1:\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.module.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "    else:\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "    torch.save(state, file_name)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adadelta, Adam, RMSprop, AdamW\n",
    "\n",
    "def get_optimizer(params, optimizer, lr=1e-4, momentum=0.9, weight_decay=0.0):\n",
    "    \"\"\"\n",
    "    Loads and returns the optimizer.\n",
    "    \"\"\"\n",
    "    if optimizer == 'SGD':\n",
    "        return SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif optimizer == 'SGD_Nesterov':\n",
    "        return SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n",
    "    elif optimizer == 'Adadelta':\n",
    "        return Adadelta(params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer == 'Adam':\n",
    "        return Adam(params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer == 'AdamW':\n",
    "        return AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer == 'RMSprop':\n",
    "        return RMSprop(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise Exception('Unknown optimizer : {}'.format(optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Code extracted from MIT licensed code Copyright (c) 2021 Alinstein Jose\n",
    "# Implementation of \"Weakly supervised Classification and Localization of Chest X-ray images\"\n",
    "def Find_Optimal_Cutoff(target, predicted):\n",
    "    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n",
    "    Parameters\n",
    "    ----------\n",
    "    target : Matrix with dependent or target data, where rows are observations\n",
    "    predicted : Matrix with predicted data, where rows are observations\n",
    "    Returns\n",
    "    -------\n",
    "    list type, with optimal cutoff value\n",
    "    \"\"\"\n",
    "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "    i = np.arange(len(tpr))\n",
    "    roc = pd.DataFrame({'tf': pd.Series(tpr - (1 - fpr), index=i), 'threshold': pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.iloc[(roc.tf - 0).abs().argsort()[:1]]\n",
    "    return list(roc_t['threshold'])\n",
    "\n",
    "\n",
    "# Idea extacted from the [https://arxiv.org/pdf/1901.05555] paper \"Class-Balanced Loss Based on Effective Number of Samples\"\n",
    "# We can normalize inverse frequencies but this method is studied to do better in other works\n",
    "def effective_weights(class_counts, beta=0.999):\n",
    "    effective_num = 1.0 - np.power(beta, class_counts)\n",
    "    weights = (1.0 - beta) / np.array(effective_num)\n",
    "    weights = weights / np.sum(weights)  # Normalize the weights -> or other(?)\n",
    "    return torch.tensor(weights, dtype=torch.float32)  # Convert to tensor\n",
    "\n",
    "\n",
    "# Code extracted from MIT licensed code Copyright (c) 2020 Alibaba-MIIL\n",
    "# Original idea in the [https://arxiv.org/pdf/2009.14119] paper \"Asymmetric Loss For Multi-Label Classification\"\n",
    "class AsymmetricLoss(nn.Module):\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=True, average=False):\n",
    "        super(AsymmetricLoss, self).__init__()\n",
    "\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
    "        self.eps = eps\n",
    "        self.average = average\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: input logits\n",
    "        y: targets (multi-label binarized vector)\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculating Probabilities\n",
    "        x_sigmoid = torch.sigmoid(x)\n",
    "        xs_pos = x_sigmoid\n",
    "        xs_neg = 1 - x_sigmoid\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            xs_neg = (xs_neg + self.clip).clamp(max=1)\n",
    "\n",
    "        # Basic CE calculation\n",
    "        los_pos = y * torch.log(xs_pos.clamp(min=self.eps))\n",
    "        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=self.eps))\n",
    "        loss = los_pos + los_neg\n",
    "\n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(False)\n",
    "            pt0 = xs_pos * y\n",
    "            pt1 = xs_neg * (1 - y)  # pt = p if t > 0 else 1-p\n",
    "            pt = pt0 + pt1\n",
    "            one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)\n",
    "            one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(True)\n",
    "            loss *= one_sided_w\n",
    "\n",
    "        # Really small change introduced by thesis author (me) to try average loss impact\n",
    "        # As discussed in \"https://github.com/Alibaba-MIIL/ASL/issues/22\" it's not suposed\n",
    "        # To have an impact but I want to be able to use mean for ease of comparison.\n",
    "        if self.average:\n",
    "            return -loss.mean()\n",
    "        else:\n",
    "            return -loss.sum()\n",
    "    \n",
    "\n",
    "# Function for testing different loss types\n",
    "def get_loss(loss_type='bce', counts=None, beta=0.99, device='cpu'):\n",
    "    \n",
    "    if loss_type == 'bce':\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "    elif loss_type == 'bce_w':\n",
    "        if counts is not None:\n",
    "            class_weights = effective_weights(counts, beta=beta).to(device)\n",
    "            loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "        else:\n",
    "            print(\"You didn't provide weights so we proceed with normal BCE\")\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "    elif loss_type == 'asymmetric':\n",
    "        loss_fn = AsymmetricLoss()\n",
    "    elif loss_type == 'asymmetric_avg':\n",
    "        loss_fn = AsymmetricLoss(average=True)\n",
    "    elif loss_type == 'asl1':\n",
    "        loss_fn = AsymmetricLoss(gamma_neg=2, gamma_pos=1, clip=0)\n",
    "    elif loss_type == 'asl2':\n",
    "        loss_fn = AsymmetricLoss(gamma_neg=4, gamma_pos=1, clip=0.05)\n",
    "    elif loss_type == 'asl3':\n",
    "        loss_fn = AsymmetricLoss(gamma_neg=8, gamma_pos=1, clip=0.05)\n",
    "    elif loss_type == 'focal':\n",
    "        loss_fn = AsymmetricLoss(gamma_neg=2, gamma_pos=2, clip=0)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss_type '{loss_type}'. Use 'bce' or 'asymmetric'.\")\n",
    "    \n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, steps=1000, patience=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())  # Keep track of best model weights\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss as infinity\n",
    "    epochs_without_improvement = 0  # Counter for epochs without improvement\n",
    "\n",
    "    # Define the learning rate scheduler (reduce LR on plateau)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "    val_losses = []  # List to store validation losses for plotting\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        print(f'Starting epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        # Training loop\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            if i >= steps:  # Break after the defined number of steps\n",
    "                break\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['labels'].to(device).float()\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation at the end of each epoch\n",
    "        val_loss = validate_model(model, val_loader, criterion)\n",
    "        val_losses.append(val_loss)  # Log the validation loss\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss  # Update best validation loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())  # Save the best model weights\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f'No improvement in validation loss for {epochs_without_improvement} epoch(s).')\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epochs_without_improvement} epochs without improvement.')\n",
    "            break\n",
    "\n",
    "        # Update the learning rate based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # Load best model weights before returning\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print('Training complete. Best Validation Loss:', best_val_loss)\n",
    "    \n",
    "    return model, val_losses  # Return the model and the validation losses\n",
    "\n",
    "def validate_model(model, val_loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['labels'].to(device).float()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)  # Return average validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test function\n",
    "\n",
    "Test function that generates the testing values that are necessary for the evaluation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    threshold = 0.5  # Threshold for multi-label classification\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            predicted = (torch.sigmoid(outputs) > threshold).float()\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for the best Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "def test_optimizer(opt_name, val_losses_storage):\n",
    "    # Hyperparameters\n",
    "    batch_size = 16\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 25\n",
    "    steps_per_epoch = 500\n",
    "    image_size = 256\n",
    "    num_classes = 15\n",
    "\n",
    "    # Paths\n",
    "    train_csv = './labels/train_metadata.csv'\n",
    "    val_csv = './labels/val_metadata.csv'\n",
    "    image_dir = './resized_images'\n",
    "\n",
    "    # Data Loaders\n",
    "    dataloaders, dataset_sizes, class_counts = make_data_loaders(train_csv, val_csv, image_dir, batch_size, image_size)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    print(f'Testing optimizer: {opt_name}')\n",
    "\n",
    "    # Model initialization\n",
    "    model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    # Get the optimizer\n",
    "    parameters = model.parameters()\n",
    "    optimizer = get_optimizer(parameters, opt_name, learning_rate)\n",
    "\n",
    "    # Train the model and get validation losses\n",
    "    _, val_losses = train_model(model, dataloaders['train'], dataloaders['val'], criterion, optimizer, num_epochs, steps_per_epoch)\n",
    "\n",
    "    # Store validation losses for this optimizer\n",
    "    val_losses_storage[opt_name] = val_losses\n",
    "\n",
    "    return val_losses_storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(val_losses, message,ylabel):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for name, val_losses in val_losses.items():\n",
    "        plt.plot(val_losses, label=f'{name}')\n",
    "\n",
    "    plt.title(message)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses_storage = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing SGD optimizer\n",
    "val_losses_storage = test_optimizer('SGD', val_losses_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing SGD optimizer\n",
    "val_losses_storage = test_optimizer('SGD_Nesterov', val_losses_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing SGD optimizer\n",
    "val_losses_storage = test_optimizer('Adadelta', val_losses_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing SGD optimizer\n",
    "val_losses_storage = test_optimizer('Adam', val_losses_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing SGD optimizer\n",
    "val_losses_storage = test_optimizer('AdamW', val_losses_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing SGD optimizer\n",
    "val_losses_storage = test_optimizer('RMSprop', val_losses_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(val_losses_storage,'Validation Loss vs Epochs for Different Optimizers','Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for the best learning rate and batch size combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def train_model_logs(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, steps=1000, patience=5):\n",
    "    model.to(device)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # Define the learning rate scheduler (reduce LR on plateau)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "    val_losses = []  # List to store validation losses for plotting\n",
    "    val_auc_scores = []  # List to store AUC scores for each epoch\n",
    "    time_per_epoch = []  # List to store elapsed time per epoch\n",
    "    lr_per_epoch = [] # List of Lr evlution per epoch\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()  # Start time for epoch timing\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        print(f'Starting epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        # Training loop\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            #print(i)\n",
    "            if i >= steps:\n",
    "                #print('break')\n",
    "                break\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['labels'].to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        elapsed_time_train = time.time() - start_time  # Calculate elapsed time\n",
    "        \n",
    "\n",
    "        # Validation at the end of each epoch\n",
    "        start_time = time.time()  # Start time for epoch timing\n",
    "        val_loss, val_auc = validate_model_logs(model, val_loader, criterion)\n",
    "\n",
    "        elapsed_time_val = time.time() - start_time  # Calculate elapsed time\n",
    "\n",
    "        val_losses.append(val_loss)  # Log the validation loss\n",
    "        val_auc_scores.append(val_auc)  # Log the AUC scores\n",
    "        time_per_epoch.append((elapsed_time_train,elapsed_time_val))  # Epoch elapsed time\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, Time on train: {elapsed_time_train:.2f}s, Time on val: {elapsed_time_val:.2f}s')\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f'No improvement in validation loss for {epochs_without_improvement} epoch(s).')\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epochs_without_improvement} epochs without improvement.')\n",
    "            break\n",
    "\n",
    "        # Update the learning rate based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        new_lr = scheduler.get_last_lr()\n",
    "        lr_per_epoch.append(new_lr)\n",
    "        print(f'Learning rate is updated to {new_lr}')\n",
    "\n",
    "\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print('Training complete. Best Validation Loss:', best_val_loss)\n",
    "\n",
    "    return model, val_losses, val_auc_scores, time_per_epoch  # Return the model and all logs\n",
    "\n",
    "def validate_model_logs(model, val_loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['labels'].to(device).float()\n",
    "\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            all_outputs.append(outputs.cpu().detach().numpy())\n",
    "            all_labels.append(labels.cpu().detach().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)  # Average validation loss\n",
    "    all_outputs = np.concatenate(all_outputs)  # Concatenate outputs for AUC calculation\n",
    "    all_labels = np.concatenate(all_labels)  # Concatenate labels for AUC calculation\n",
    "\n",
    "    # Calculate AUC for each label\n",
    "    auc_scores = []\n",
    "    for i in range(all_labels.shape[1]):  # Assuming all_labels is shape [num_samples, num_labels]\n",
    "        if np.unique(all_labels[:, i]).size > 1:  # Check for both classes in the label\n",
    "            auc = roc_auc_score(all_labels[:, i], all_outputs[:, i])\n",
    "            auc_scores.append(auc)\n",
    "        else:\n",
    "            auc_scores.append(np.nan)  # If only one class is present, AUC is undefined\n",
    "\n",
    "    mean_auc = np.nanmean(auc_scores)  # Calculate mean AUC ignoring NaN values\n",
    "    return val_loss, mean_auc  # Return average validation loss and mean AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "# Hyperparameters\n",
    "batch_sizes = [32, 64, 128, 256, 512]\n",
    "learning_rates = [1e-4, 5e-4, 1e-3, 1e-2, 1e-1]\n",
    "num_epochs = 15\n",
    "iter_per_epoch = 8192\n",
    "image_size = 256\n",
    "num_classes = 15\n",
    "\n",
    "# Paths\n",
    "train_csv = './labels/train_metadata_positive.csv'\n",
    "val_csv = './labels/val_metadata_positive.csv'\n",
    "image_dir = './resized_images'\n",
    "\n",
    "# Storage matrices\n",
    "final_val_losses = np.zeros((len(batch_sizes), len(learning_rates)))\n",
    "final_val_aucs = np.zeros((len(batch_sizes), len(learning_rates)))\n",
    "epoch_times_matrix = np.zeros((len(batch_sizes), len(learning_rates)))\n",
    "\n",
    "# Function to map batch_size and lr indices\n",
    "def get_index(batch_size, lr):\n",
    "    batch_index = batch_sizes.index(batch_size)\n",
    "    lr_index = learning_rates.index(lr)\n",
    "    return batch_index, lr_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each combination of batch size and learning rate\n",
    "for batch_size in [32, 64]:\n",
    "    for lr in learning_rates:\n",
    "        print(f'Starting test with batch size = {batch_size} and learning rate = {lr}')\n",
    "\n",
    "        # Data Loaders\n",
    "        dataloaders, dataset_sizes, class_counts = make_data_loaders(train_csv, val_csv, image_dir, batch_size, image_size)\n",
    "\n",
    "        # Model setup\n",
    "        model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = get_optimizer(model.parameters(), 'Adam', lr)\n",
    "\n",
    "        final_epoch_iter = iter_per_epoch/batch_size\n",
    "\n",
    "        print(f'The final epoch iterations are {final_epoch_iter}')\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Train the model and log results\n",
    "        model, val_losses, val_aucs, epoch_times = train_model_logs(device, model, dataloaders['train'], dataloaders['val'], criterion, optimizer, num_epochs, final_epoch_iter)\n",
    "\n",
    "        # Get the index for storing the results in the matrix\n",
    "        batch_index, lr_index = get_index(batch_size, lr)\n",
    "\n",
    "        # Store the final validation loss, mean AUC, and average epoch time\n",
    "        final_val_losses[batch_index, lr_index] = val_losses[-1]  # Final validation loss of the last epoch\n",
    "        final_val_aucs[batch_index, lr_index] = val_aucs[-1]  # Final AUC of the last epoch\n",
    "        epoch_times_matrix[batch_index, lr_index] = np.mean(epoch_times)  # Mean time per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the final matrices\n",
    "print(\"\\nFinal Validation Losses (Rows = Batch Sizes, Cols = Learning Rates):\")\n",
    "print(final_val_losses)\n",
    "\n",
    "print(\"\\nFinal Validation AUCs (Rows = Batch Sizes, Cols = Learning Rates):\")\n",
    "print(final_val_aucs)\n",
    "\n",
    "print(\"\\nEpoch Times (Rows = Batch Sizes, Cols = Learning Rates):\")\n",
    "print(epoch_times_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each combination of batch size and learning rate\n",
    "for batch_size in [128, 256]:\n",
    "    for lr in learning_rates:\n",
    "        print(f'Starting test with batch size = {batch_size} and learning rate = {lr}')\n",
    "\n",
    "        # Data Loaders\n",
    "        dataloaders, dataset_sizes, class_counts = make_data_loaders(train_csv, val_csv, image_dir, batch_size, image_size)\n",
    "\n",
    "        # Model setup\n",
    "        model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = get_optimizer(model.parameters(), 'Adam', lr)\n",
    "\n",
    "        final_epoch_iter = iter_per_epoch/batch_size\n",
    "\n",
    "        print(f'The final epoch iterations are {final_epoch_iter}')\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Train the model and log results\n",
    "        model, val_losses, val_aucs, epoch_times = train_model_logs(device, model, dataloaders['train'], dataloaders['val'], criterion, optimizer, num_epochs, final_epoch_iter)\n",
    "\n",
    "        # Get the index for storing the results in the matrix\n",
    "        batch_index, lr_index = get_index(batch_size, lr)\n",
    "\n",
    "        # Store the final validation loss, mean AUC, and average epoch time\n",
    "        final_val_losses[batch_index, lr_index] = val_losses[-1]  # Final validation loss of the last epoch\n",
    "        final_val_aucs[batch_index, lr_index] = val_aucs[-1]  # Final AUC of the last epoch\n",
    "        epoch_times_matrix[batch_index, lr_index] = np.mean(epoch_times)  # Mean time per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each combination of batch size and learning rate\n",
    "for batch_size in [512]:\n",
    "    for lr in learning_rates:\n",
    "        print(f'Starting test with batch size = {batch_size} and learning rate = {lr}')\n",
    "\n",
    "        # Data Loaders\n",
    "        dataloaders, dataset_sizes, class_counts = make_data_loaders(train_csv, val_csv, image_dir, batch_size, image_size)\n",
    "\n",
    "        # Model setup\n",
    "        model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = get_optimizer(model.parameters(), 'Adam', lr)\n",
    "\n",
    "        final_epoch_iter = iter_per_epoch/batch_size\n",
    "\n",
    "        print(f'The final epoch iterations are {final_epoch_iter}')\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Train the model and log results\n",
    "        model, val_losses, val_aucs, epoch_times = train_model_logs(device, model, dataloaders['train'], dataloaders['val'], criterion, optimizer, num_epochs, final_epoch_iter)\n",
    "\n",
    "        # Get the index for storing the results in the matrix\n",
    "        batch_index, lr_index = get_index(batch_size, lr)\n",
    "\n",
    "        # Store the final validation loss, mean AUC, and average epoch time\n",
    "        final_val_losses[batch_index, lr_index] = val_losses[-1]  # Final validation loss of the last epoch\n",
    "        final_val_aucs[batch_index, lr_index] = val_aucs[-1]  # Final AUC of the last epoch\n",
    "        epoch_times_matrix[batch_index, lr_index] = np.mean(epoch_times)  # Mean time per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the final matrices\n",
    "print(\"\\nFinal Validation Losses (Rows = Batch Sizes, Cols = Learning Rates):\")\n",
    "print(final_val_losses)\n",
    "\n",
    "print(\"\\nFinal Validation AUCs (Rows = Batch Sizes, Cols = Learning Rates):\")\n",
    "print(final_val_aucs)\n",
    "\n",
    "print(\"\\nEpoch Times (Rows = Batch Sizes, Cols = Learning Rates):\")\n",
    "print(epoch_times_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 5e-4\n",
    "num_epochs = 10\n",
    "iter_per_epoch = 500\n",
    "image_sizes = [256,224,128,64]\n",
    "num_classes = 15\n",
    "\n",
    "# Paths\n",
    "train_csv = './labels/train_metadata.csv'\n",
    "val_csv = './labels/val_metadata.csv'\n",
    "image_dir = './resized_images'\n",
    "\n",
    "# Storage matrices\n",
    "final_val_losses = []\n",
    "final_val_aucs = []\n",
    "final_epoch_times = []\n",
    "\n",
    "# Function to map batch_size and lr indices\n",
    "def get_index(batch_size, lr):\n",
    "    batch_index = batch_sizes.index(batch_size)\n",
    "    lr_index = learning_rates.index(lr)\n",
    "    return batch_index, lr_index\n",
    "\n",
    "# Test each combination of batch size and learning rate\n",
    "for image_size in image_sizes:\n",
    "    print(f'Starting test with image size = {image_size}')\n",
    "\n",
    "    # Data Loaders\n",
    "    dataloaders, dataset_sizes, class_counts = make_data_loaders(train_csv, val_csv, image_dir, batch_size, image_size)\n",
    "\n",
    "    # Model setup\n",
    "    model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = get_optimizer(model.parameters(), 'Adam', learning_rate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Train the model and log results\n",
    "    model, val_losses, val_aucs, epoch_times = train_model_logs(device, model, dataloaders['train'], dataloaders['val'], criterion, optimizer, num_epochs, iter_per_epoch)\n",
    "\n",
    "    final_val_losses.append(val_losses)\n",
    "    final_val_aucs.append(val_aucs)\n",
    "    final_epoch_times.append(epoch_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_exp = {}\n",
    "for i in range(len(image_sizes)):\n",
    "    size_exp[image_sizes[i]] = final_val_losses[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(size_exp, 'Validation Loss vs Epochs for different Image sizes','Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_model_history(device, model, train_loader, val_loader, criterion, optimizer, num_epochs, steps=1000, patience=5):\n",
    "    model.to(device)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # Define the learning rate scheduler (reduce LR on plateau)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "    # DataFrame to store history\n",
    "    history = pd.DataFrame(columns=['epoch', 'val_loss', 'val_auc', 'precision', 'recall', 'f1_score', 'lr', 'train_time', 'val_time', 'epoch_time'])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        print(f'Starting epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        start_time = time.time()  # Start time for training phase\n",
    "\n",
    "        # Training loop\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            if i >= steps:\n",
    "                break\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['labels'].to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_time = time.time() - start_time  # End time for training phase\n",
    "\n",
    "        # Validation at the end of each epoch\n",
    "        start_time_val = time.time()  # Start time for validation phase\n",
    "\n",
    "        val_loss, val_auc, val_precision, val_recall, val_f1 = validate_model_history(model, val_loader, criterion)\n",
    "\n",
    "        val_time = time.time() - start_time_val  # End time for validation phase\n",
    "\n",
    "        epoch_time = time.time() - start_time  # End time for the entire epoch\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}, AUC: {val_auc:.4f}, '\n",
    "              f'Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1-score: {val_f1:.4f}, '\n",
    "              f'Training Time: {train_time:.2f}s, Validation Time: {val_time:.2f}s, Total Time: {epoch_time:.2f}s')\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f'No improvement in validation loss for {epochs_without_improvement} epoch(s).')\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epochs_without_improvement} epochs without improvement.')\n",
    "            break\n",
    "\n",
    "        # Update the learning rate based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Log metrics into the DataFrame, including time information\n",
    "        history = pd.concat([history, pd.DataFrame({\n",
    "            'epoch': [epoch + 1],\n",
    "            'val_loss': [val_loss],\n",
    "            'val_auc': [val_auc],\n",
    "            'precision': [val_precision],\n",
    "            'recall': [val_recall],\n",
    "            'f1_score': [val_f1],\n",
    "            'lr': [current_lr],\n",
    "            'train_time': [train_time],\n",
    "            'val_time': [val_time],\n",
    "            'epoch_time': [epoch_time]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "        print(f'Learning rate is updated to {current_lr}')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print('Training complete. Best Validation Loss:', best_val_loss)\n",
    "\n",
    "    return model, history  # Return the model and the history DataFrame\n",
    "\n",
    "def validate_model_history(model, val_loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['labels'].to(device).float()\n",
    "\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            all_outputs.append(torch.sigmoid(outputs).cpu().detach().numpy())  # Apply sigmoid\n",
    "            all_labels.append(labels.cpu().detach().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)  # Average validation loss\n",
    "    all_outputs = np.concatenate(all_outputs)  # Concatenate outputs for AUC calculation\n",
    "    all_labels = np.concatenate(all_labels)  # Concatenate labels for AUC calculation\n",
    "\n",
    "    # Threshold outputs for binary predictions\n",
    "    all_preds = (all_outputs > 0.5).astype(int)\n",
    "\n",
    "    # Calculate AUC for each label\n",
    "    auc_scores = []\n",
    "    for i in range(all_labels.shape[1]):  # Assuming all_labels is shape [num_samples, num_labels]\n",
    "        if np.unique(all_labels[:, i]).size > 1:  # Check for both classes in the label\n",
    "            auc = roc_auc_score(all_labels[:, i], all_outputs[:, i])\n",
    "            auc_scores.append(auc)\n",
    "        else:\n",
    "            auc_scores.append(np.nan)  # If only one class is present, AUC is undefined\n",
    "\n",
    "    mean_auc = np.nanmean(auc_scores)  # Calculate mean AUC ignoring NaN values\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(all_labels, all_preds, average='micro')\n",
    "    recall = recall_score(all_labels, all_preds, average='micro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "\n",
    "    return val_loss, mean_auc, precision, recall, f1  # Return all metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 5e-4\n",
    "num_epochs = 5\n",
    "iter_per_epoch = 50\n",
    "image_size = 256\n",
    "num_classes = 15\n",
    "\n",
    "# Paths\n",
    "train_csv = './labels/train_metadata.csv'\n",
    "val_csv = './labels/val_metadata.csv'\n",
    "image_dir = './resized_images'\n",
    "\n",
    "# Data Loaders\n",
    "dataloaders, dataset_sizes, class_counts = make_data_loaders(train_csv, val_csv, image_dir, batch_size, image_size)\n",
    "\n",
    "print(class_counts)\n",
    "print(effective_weights(class_counts))\n",
    "print(torch.sum(effective_weights(class_counts)))\n",
    "\n",
    "# Model setup\n",
    "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "counts = np.array(class_counts)\n",
    "loss_fn = get_loss(loss_type='asymmetric', counts=counts, device=device)\n",
    "print(loss_fn)\n",
    "optimizer = get_optimizer(model.parameters(), 'Adam', learning_rate)\n",
    "\n",
    "\n",
    "# Train the model and log results\n",
    "model, history = train_model_history(device, model, dataloaders['train'], dataloaders['val'], loss_fn, optimizer, num_epochs, iter_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert pred and labels to PyTorch tensors\n",
    "pred_tensor = torch.tensor([[-7.3645e-01,  5.7881e-01, -2.2020e-01, -5.1094e-01,  2.9868e-01,\n",
    "                             2.6686e-01,  3.5119e-02, -6.2442e-01, -2.2921e-01,  2.8613e-01,\n",
    "                             3.6774e-01, -4.4509e-01, -1.0131e+00,  2.7856e-02,  1.0464e+00],\n",
    "                            [ 5.1233e-01, -2.9292e-02, -8.7824e-02, -8.9985e-01,  6.1635e-01,\n",
    "                             1.0113e+00,  1.2067e-02, -1.3016e+00,  2.0202e-01, -3.5548e-01,\n",
    "                            -5.4201e-01, -5.7460e-01, -4.4881e-01,  9.9741e-01,  1.1251e+00],\n",
    "                            [ 5.9710e-01,  6.8125e-02, -2.9039e-01, -7.9447e-01,  5.1311e-01,\n",
    "                            -3.1247e-01, -1.2926e-01, -1.1064e+00, -6.4016e-01, -9.4150e-01,\n",
    "                             1.9711e-01, -4.3601e-01, -6.0893e-01,  6.6210e-02,  5.9627e-01],\n",
    "                            [-1.4584e-01,  1.2339e-01, -9.8124e-02, -5.5506e-01,  6.5351e-01,\n",
    "                             3.3160e-01, -1.7156e-01, -5.0052e-01, -3.9164e-01,  2.6694e-01,\n",
    "                             5.2570e-01,  2.9331e-02, -7.3786e-01,  1.6505e-01,  8.5180e-01],\n",
    "                            [ 3.7813e-01,  1.6255e-01, -1.9692e-01, -5.3570e-01,  8.5489e-01,\n",
    "                             1.0920e+00,  3.0120e-01, -9.9126e-01, -6.8351e-02, -5.0176e-01,\n",
    "                            -5.2612e-01, -7.2143e-01, -6.2451e-01,  5.1360e-01,  1.0608e+00],\n",
    "                            [ 7.1772e-01,  5.5271e-01, -3.9907e-01, -1.2032e+00,  5.2024e-01,\n",
    "                             2.2213e-01, -7.2604e-02, -1.0538e+00, -3.7901e-01, -5.6711e-01,\n",
    "                             2.4622e-01, -3.0617e-01, -2.3560e-02,  1.0056e-01,  7.4420e-01]])\n",
    "\n",
    "labels_tensor = torch.tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                              [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                              [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.],\n",
    "                              [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "                              [0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "                              [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
    "\n",
    "# Paths\n",
    "train_csv = './labels/train_metadata.csv'\n",
    "val_csv = './labels/val_metadata.csv'\n",
    "image_dir = './resized_images'\n",
    "\n",
    "print(f'Starting test with bce')\n",
    "\n",
    "# Data Loaders\n",
    "dataloaders, dataset_sizes, class_counts = make_data_loaders(train_csv, val_csv, image_dir, batch_size, image_size)\n",
    "\n",
    "print(class_counts)\n",
    "print(effective_weights(class_counts))\n",
    "print(torch.sum(effective_weights(class_counts)))\n",
    "\n",
    "# Loss function and optimizer\n",
    "counts = np.array(class_counts)\n",
    "for loss in ['asymmetric','focal','bce_w','bce']:\n",
    "    loss_fn = get_loss(loss_type=loss, counts=counts)\n",
    "    print(loss_fn)\n",
    "\n",
    "    curr_loss = loss_fn(pred_tensor, labels_tensor)\n",
    "    print(f\"{loss} Loss: {curr_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing different loss types\n",
    "def get_loss(loss_type='bce', counts=None, beta=0.99, device='cpu'):\n",
    "    \n",
    "    if loss_type == 'bce':\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "    elif loss_type == 'bce_w':\n",
    "        if counts is not None:\n",
    "            class_weights = effective_weights(counts, beta=beta).to(device)\n",
    "            loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "        else:\n",
    "            print(\"You didn't provide weights so we proceed with normal BCE\")\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "    elif loss_type == 'asymmetric':\n",
    "        loss_fn = AsymmetricLoss()\n",
    "    elif loss_type == 'asymmetric_avg':\n",
    "        loss_fn = AsymmetricLoss(average=True)\n",
    "    elif loss_type == 'asl1':\n",
    "        loss_fn = AsymmetricLoss(gamma_neg=2, gamma_pos=1, clip=0)\n",
    "    elif loss_type == 'asl2':\n",
    "        loss_fn = AsymmetricLoss(gamma_neg=4, gamma_pos=1, clip=0.05)\n",
    "    elif loss_type == 'asl3':\n",
    "        loss_fn = AsymmetricLoss(gamma_neg=8, gamma_pos=1, clip=0.05)\n",
    "    elif loss_type == 'asl4':\n",
    "        loss_fn = AsymmetricLoss(gamma_neg=2, gamma_pos=1, clip=0.025)\n",
    "    elif loss_type == 'asl5':\n",
    "        loss_fn = AsymmetricLoss(gamma_neg=4, gamma_pos=1, clip=0)\n",
    "    elif loss_type == 'focal':\n",
    "        loss_fn = AsymmetricLoss(gamma_neg=2, gamma_pos=2, clip=0)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss_type '{loss_type}'. Use 'bce' or 'asymmetric'.\")\n",
    "    \n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 5e-4\n",
    "num_epochs = 25\n",
    "iter_per_epoch = 64\n",
    "image_size = 256\n",
    "num_classes = 15\n",
    "\n",
    "# Paths\n",
    "train_csv = './labels/train_metadata.csv'\n",
    "val_csv = './labels/val_metadata.csv'\n",
    "image_dir = './resized_images'\n",
    "\n",
    "# Storage matrices\n",
    "histories = {}\n",
    "\n",
    "\n",
    "# Test each combination of batch size and learning rate\n",
    "for loss_func in ['bce','asymmetric','asl1','asl2','asl4','asl5']: # \n",
    "    print(f'Starting test with loss = {loss_func}')\n",
    "\n",
    "    # Data Loaders\n",
    "    dataloaders, dataset_sizes, class_counts = make_data_loaders(train_csv, val_csv, image_dir, batch_size, image_size)\n",
    "\n",
    "    # Model setup\n",
    "    model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    counts = np.array(class_counts)\n",
    "    loss_fn = get_loss(loss_type=loss_func, counts=counts, device=device)\n",
    "    optimizer = get_optimizer(model.parameters(), 'Adam', learning_rate)\n",
    "\n",
    "    # Train the model and log results\n",
    "    model, history = train_model_history(device, model, dataloaders['train'], dataloaders['val'], loss_fn, optimizer, num_epochs, iter_per_epoch)\n",
    "\n",
    "    histories[loss_func] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in histories.items():\n",
    "    # Save each DataFrame as \"key.csv\"\n",
    "    filename = f\"./tests/loss_func/{key}.csv\"\n",
    "    df.to_csv(filename, index=False)  # Set index=False if you don't want to store the index in the CSV\n",
    "    print(f\"Saved {key} as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {}\n",
    "for key in ['bce','bce_w','focal','asymmetric','asymmetric_avg','asl1','asl2','asl3']:\n",
    "    # Save each DataFrame as \"key.csv\"\n",
    "    filename = f\"./tests/loss_func/{key}.csv\"\n",
    "    histories[key] = pd.read_csv(filename)  \n",
    "    print(f\"Retrieved {key} from {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(history_dict):\n",
    "    # Create 4 subplots for AUC, Precision, Recall, and F1 score\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(12, 20))\n",
    "    \n",
    "    # Plot for each key in the dictionary\n",
    "    for key, history in history_dict.items():\n",
    "        epochs = history['epoch']\n",
    "        \n",
    "        # Plot AUC\n",
    "        axs[0].plot(epochs, history['val_auc'], label=key)\n",
    "        axs[0].set_title('AUC per Epoch')\n",
    "        axs[0].set_xlabel('Epoch')\n",
    "        axs[0].set_ylabel('AUC')\n",
    "        \n",
    "        # Plot Precision\n",
    "        axs[2].plot(epochs, history['precision'], label=key)\n",
    "        axs[2].set_title('Precision per Epoch')\n",
    "        axs[2].set_xlabel('Epoch')\n",
    "        axs[2].set_ylabel('Precision')\n",
    "        \n",
    "        # Plot Recall\n",
    "        axs[1].plot(epochs, history['recall'], label=key)\n",
    "        axs[1].set_title('Recall per Epoch')\n",
    "        axs[1].set_xlabel('Epoch')\n",
    "        axs[1].set_ylabel('Recall')\n",
    "        \n",
    "        # Plot F1 Score\n",
    "        axs[3].plot(epochs, history['f1_score'], label=key)\n",
    "        axs[3].set_title('F1 Score per Epoch')\n",
    "        axs[3].set_xlabel('Epoch')\n",
    "        axs[3].set_ylabel('F1 Score')\n",
    "    \n",
    "    # Add legend to all plots\n",
    "    for ax in axs.flat:\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(histories)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_dev_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
